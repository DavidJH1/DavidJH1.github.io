---
title: "Auto-Charting Guitar Hero Songs with Machine Learning"
subtitle: "Building a model that listens to music and generates Clone Hero charts"
author: "David Hansen"
date: today
categories: [Machine Learning, Audio Analysis, Clone Hero, Deep Learning]
format:
  html:
    toc: true
    code-copy: true
    code-overflow: wrap
    fig-cap-location: bottom
---

## Background

Guitar Hero has been around since the early 2000s, inspiring a passionate fanbase. One of the community’s biggest contributions is **Clone Hero**, a free PC-based version of the game that supports custom songs. Alongside Clone Hero, fans have built tools to help manually chart songs — a process that can take many hours for a single track.

Each full chart typically includes multiple instruments and difficulty levels. For example, a guitar, bass, and drums chart across four difficulties adds up to **15+ charts per song**, not counting vocals. It’s incredibly time-consuming.

The idea behind this project is simple but ambitious: **build a model that can listen to a song and predict which Guitar Hero notes should go where.**

## Data Collection

To train a model like this, I needed two key things:

- The **song audio**
- The **Guitar Hero chart** (specifically the expert guitar part)

Fortunately, the Clone Hero community has manually charted thousands of songs and shares them freely online. I used the site **Chorus Encore** to download songs and their corresponding `.chart` files. [Chorus](https://www.enchor.us/)

For the scope of this initial model, I focused only on **expert guitar charts** to reduce complexity and eliminate other instruments and difficulties.

## Data Processing

Computers don’t have ears — so to help them "listen" to music, we must convert sound into numerical data.

### Step 1: Convert audio to waveforms

Using [Librosa](https://librosa.org/), I first transformed each audio file into **waveform data**.

```python
import librosa
y, sr = librosa.load("song.mp3")
librosa.display.waveshow(y, sr=sr)
```
![Wave Data](Images\wave_full.png)


### Step 2: Generate Spectrograms

While waveform data is a start, it doesn’t capture musical structure clearly. To extract meaningful audio features, we convert waveforms into **spectrograms**.

A **spectrogram** shows frequency (y-axis) over time (x-axis), with amplitude represented by color intensity (measured in decibels). But raw spectrograms don’t reflect human hearing very well.

![Normal Spectrogram](Images\spectrogram_full.png)

To address that, we use a **Mel spectrogram**, which spaces frequencies on a perceptual scale that better matches how we hear sound — emphasizing low frequencies more than high ones.

```python
import librosa
import librosa.display
import matplotlib.pyplot as plt

y, sr = librosa.load("song.mp3", sr=None)
S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
S_dB = librosa.power_to_db(S, ref=np.max)

plt.figure(figsize=(10, 4))
librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')
plt.colorbar(format='%+2.0f dB')
plt.title('Mel Spectrogram')
plt.tight_layout()
plt.show()
```

![Mel Spectrogram](Images\melspec_full.png)


### Step 3: Aligning Audio with Chart Data

The next major step was to **align the audio data with Guitar Hero note events** from the `.chart` files.

Each `.chart` file contains a structured list of musical events, like so:

```text
6528 = N 1 0
6528 = N 6 0

```

Let’s break this down:
- **6528** is the **tick** — a unit of time used by the game engine.
- **N** means it's a **note** event.
- **1** represents the **note type** (0 = green, 1 = red, 2 = yellow, 3 = blue, 4 = orange).
- **6** indicates a **tap note flag** (5 = HOPO, 6 = tap, 7 = open note).
- **0** is the **note duration** in ticks (for sustains).

At tick 6528, the chart defines a red note (`1`) that is also a tap note (`6`).

---

To turn this into data the model can use, I needed to:

1. **Filter only the expert guitar part** from the chart file.
2. **Convert ticks to seconds** using tempo and resolution metadata.
3. **Slice the Mel spectrogram** into fixed-size time windows (e.g., 32nd notes).
4. **Assign a label** to each slice based on which note (if any) occurs during that time.

Each time slice of the Mel spectrogram becomes a training input, and the corresponding chart label becomes the target output.

For instance, if a slice covers 45.3 to 45.35 seconds of the song and a red note begins at 45.32, that slice would be labeled `"red"`.

---

To visualize this, I wrote a helper function to **step through the Mel spectrogram** and grab 12-pixel-wide image slices at specific timestamps. Here's an example:

![Mel Slice with Note](Images\melspec_slice.png)

These slices serve as the **training examples** for the model.

If no note occurred during a time slice, I labeled it as `"no note"`.

---

#### A Note on Complexity

One tricky part of this process is that **multiple note events can happen at the same tick**. For example:

```text
6528 = N 0 0
6528 = N 1 0
6528 = N 2 0
6528 = N 5 0

```

This could represent a **chord of green-red-yellow**, flagged as a HOPO. So, some game events need to be **grouped** or **combined** into a single label.

Additionally, a single in-game note can span **multiple lines** in the `.chart` file — including note, modifier (tap, HOPO), and sustain.

I created a function to:
- Collapse multiple events into a **single charted note**.
- Convert the tick to **absolute time** (in seconds).
- Attach that note to a **specific spectrogram window**.

With this mapping complete, I had a labeled dataset of Mel spectrogram windows aligned to actual in-game notes — ready for model training.

## Model Training

With the Mel spectrogram slices labeled and aligned to the chart data, I moved on to training the machine learning model.

### Choosing a Model Architecture

Since Mel spectrograms resemble grayscale images, I began with a **Convolutional Neural Network (CNN)**. CNNs are well-suited to capturing local patterns in image-like data and have been successfully applied in audio classification tasks like speech recognition and music genre detection.

Each spectrogram slice was treated like a mini image — with frequency on one axis and time on the other.

### Training Strategy

Rather than training the model on the full dataset at once, I adopted a **song-by-song training strategy**. This allowed the model to focus on one musical style at a time and reduced memory usage.

Key training details:
- **Input**: 12-pixel-wide Mel spectrogram slices
- **Output**: A class label for each slice (e.g., `green`, `red`, `yellow`, `no note`, etc.)
- **Batching**: Processed one song at a time
- **Dataset**: 127 songs from various genres

### Results and Observations

After training the CNN on 127 songs, the results were mixed at best.

The model showed some early signs of pattern recognition, but it struggled with accuracy and diversity in its predictions:

- **Limited Note Variety**: In many test cases, the model predicted only one or two note types (e.g., everything became `yellow tap` or `blue strum`).
- **Sparse Multi-Note Detection**: On one occasion, it predicted a few **blue-yellow double strums**, but this wasn't consistent.
- **Overfitting to Common Patterns**: The model learned to favor dominant notes in the dataset — usually mid-range frets — but ignored less common patterns like open notes or chords.
- **Predicting the Average**: This to me showed the model had no real confindence for any specific note and basically guessed the average for all notes

These issues persisted across multiple model architectures and tuning attempts.

---

### Why the Model Struggled

There are several reasons I identified that might cause a CNN to struggle to chart effectively:

#### 1. **Guitar Hero Notes Are Symbolic, Not Absolute**

In Guitar Hero, fret buttons (green, red, yellow, etc.) don’t correspond to specific frequencies or notes like a piano key would. Instead, they represent **relative musical positions** based on gameplay feel and visual clarity.

For example:
- A fast guitar run might use `green → orange` repeatedly, even as the actual pitch moves across multiple octaves.
- A "green note" could sound completely different depending on the context and instrument tone.

This makes the task less like audio classification and more like **musical pattern modeling**.

#### 2. **Lack of Temporal Awareness**

CNNs excel at capturing local spatial patterns but lack memory. They view each spectrogram slice **independently**, with no sense of what came before or after.

But musical structure is **sequential**:
- Notes depend on what came before (e.g., a tap note follows a strummed note).
- Chord progressions, scales, and rhythms span multiple time slices.

Without a mechanism for tracking time, the model **misses the “feel”** of the song.

#### 3. **Chart Label Noise and Inconsistency**

Even though I filtered for expert guitar charts, many charts downloaded from the Clone Hero community varied in quality:

- Some were poorly timed.
- Others were incomplete or inconsistently charted.
- Some songs had duplicate charts or strange timing quirks.

This creates **label noise** — a known challenge for training supervised models. Garbage in, garbage out.

#### 4. **Audio Quality and Genre Variety**

The songs came from many different sources, formats, and genres. Some were high-quality studio tracks, others were fan edits or remixes with:
- Loud mixing
- Distorted guitars
- Missing instrumental separation

This variability made it harder for the model to find **consistent, transferable audio patterns**.

---

### Summary

Despite preprocessing and careful alignment, the CNN model failed to generalize across songs. It could latch onto local features, but it lacked the ability to:
- Track musical phrasing
- Understand rhythm and context
- Adapt to different genres or artists

This project revealed the **limits of CNNs for symbolic music generation**, especially in cases where labels are abstract and context-dependent — like charting Guitar Hero notes from raw audio.

## Next Steps

While the CNN-based model had limitations, this project revealed exciting opportunities for improvement and future exploration.

Here are the top areas I’m planning to pursue:

### 1. Switch to a Sequence-Based Model

Since music, and Guitar Hero charts, are **inherently sequential**, a model that captures **temporal context** is better suited to this task.

Options include:
- **Recurrent Neural Networks (RNNs)** like LSTMs or GRUs
- **Temporal Convolutional Networks (TCNs)**
- **Transformer architectures** like those used in music generation or transcription

These models can "remember" previous inputs and identify patterns across time, which is essential for understanding rhythm, phrasing, and chart structure.

### 2. Improve Tick-to-Time Alignment

A small timing mismatch between chart ticks and audio slices could introduce major labeling errors. I'm planning to:
- Review Clone Hero's tick and tempo resolution more precisely
- Cross-reference tick timing with actual beatmaps
- Validate against hand-labeled timestamps

Even a 50ms misalignment can cause the model to associate the wrong note with the wrong audio.

### 3. Clean the Dataset

Better data = better model.

Future iterations will focus on:
- **Filtering high-quality charts only**
- Possibly limiting to **a single band or genre** (e.g., Metallica, punk rock, pop punk)
- Using **high-resolution, properly mastered audio files**

Charting is already subjective, and bad labels make the model's job nearly impossible. Reducing label noise is critical.

### 4. Expand Label Space or Use Multi-Label Classification

Right now, the model assumes a single label per audio slice, which fails in the case of **chords** or **overlapping notes**.
I could atempt to organize and add labels to **measures or phrases** instead of labeling individual notes.

A future model could:
- Use a **multi-label approach** (e.g., outputting `[1, 0, 1, 0, 0]` for green + yellow)
- Predict **event types** like tap, HOPO, sustain, etc.
- Consider hierarchical prediction (e.g., first “note present,” then “which note(s)”)

### 5. Add Onset Detection or Beat-Synced Windows

Instead of fixed time windows (e.g., every 0.05 seconds), I could:
- Align slices to **beats or subdivisions** using a beat tracker
- Use **onset detection** to cut at transients (e.g., the start of each note)

This would make slices more musically meaningful and reduce label ambiguity.

---

## Conclusion

This project explored what it takes to build an **auto-charter for Guitar Hero songs** — a model that listens to music and outputs note charts.

Key accomplishments:
- Built a pipeline to convert audio into Mel spectrograms
- Parsed `.chart` files and aligned game notes to audio
- Trained and evaluated a CNN model on 127 custom songs
- Identified core challenges in symbolic audio modeling

The model didn’t fully succeed — but it laid the groundwork for what a future system could become. With better timing alignment, more consistent labels, and a sequence-aware model, it's entirely possible to generate accurate, playable Guitar Hero charts from real songs.

This is just the first iteration, and I’m excited to keep building.

---
